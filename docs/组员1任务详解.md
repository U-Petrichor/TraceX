# 组员1任务详解：主机日志采集

## 任务概览
负责 Linux 主机日志的采集、解析与标准化，重点关注系统审计日志（Auditd）和系统日志（Syslog）。

## 1. Filebeat 安装与配置

主机日志采集依赖 Filebeat 将本地日志（Auditd, Syslog）发送到 Elasticsearch。

### 1. Filebeat 安装与配置

### 1.1 安装方式：Docker 容器

为了保证环境的一致性，我们采用 Docker 容器方式运行 Filebeat，而非直接安装在宿主机。

### 1.2 配置步骤

更新 `docker-compose.yml`，Filebeat 会作为其中的一个服务自动启动。

**您只需要执行以下操作：**

1.  **确保 Docker 环境正常**
    ```bash
    # 在项目根目录下
    docker-compose up -d
    ```
    这会自动拉取并启动 `filebeat-collector` 容器。

2.  **配置原理 (已自动处理)**
    *   **配置文件**: 我们创建了 `collector/host_collector/filebeat.yml`，它会被挂载到容器内。
    *   **日志挂载**: 宿主机的 `/var/log` 目录被挂载到容器的 `/hostfs/var/log`，以便 Filebeat 读取宿主机的 auditd 日志。

3.  **验证状态**
    ```bash
    docker ps | grep filebeat
    # 应该能看到 filebeat-collector 正在运行
    ```

### 1.3 验证采集

运行测试脚本，它会模拟主机操作并检查数据是否入库：

```bash
# 备份原配置
sudo mv /etc/filebeat/filebeat.yml /etc/filebeat/filebeat.yml.bak

# 写入新配置 (请确保缩进正确)
sudo tee /etc/filebeat/filebeat.yml > /dev/null <<EOF
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/audit/audit.log
  pipeline: "auditd-pipeline"

- type: log
  enabled: true
  paths:
    - /var/log/syslog
    - /var/log/messages
  pipeline: "syslog-pipeline"

setup.ilm.enabled: false
setup.template.name: "unified-logs"
setup.template.pattern: "unified-logs-*"
setup.template.overwrite: true

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "unified-logs-%{+yyyy.MM.dd}"
EOF
```

### 1.3 启动服务

```bash
sudo systemctl daemon-reload
sudo systemctl enable filebeat
sudo systemctl start filebeat

# 检查状态
sudo systemctl status filebeat
```

## 2. Windows 事件日志解析支持 (新增)

为了支持跨平台日志采集，我们在 `collector/host_collector/log_parser.py` 中扩展了对 Windows Event Log (JSON 格式) 的解析支持。

### 2.1 支持的事件类型

目前支持以下三种核心安全事件的解析与映射：

| EventID | 事件类型 | 映射到 UnifiedEvent (Schema) | 关键字段提取 |
| :--- | :--- | :--- | :--- |
| **4624** | 登录成功 | `event.category="authentication"`<br>`event.action="login"` | `user.name` (TargetUserName)<br>`source.ip` (IpAddress) |
| **4688** | 进程创建 | `event.category="process"`<br>`event.action="process_created"` | `process.executable` (NewProcessName)<br>`process.command_line` (CommandLine)<br>`process.pid` (ProcessId) |
| **4663** | 对象访问 | `event.category="file"`<br>`event.action="access"` | `file.path` (ObjectName)<br>`file.name` (文件名) |

### 2.2 代码实现逻辑

*   **统一入口**: `HostLogParser.parse(raw_data, log_type="windows")`
*   **兼容性**: 保留了原有的 `auditd` 解析逻辑，通过 `log_type` 参数区分。
*   **数据清洗**: 自动处理 Windows XML/JSON 中的 `0x` 十六进制 PID 和嵌套的时间戳字段。

### 2.3 测试验证

我们提供了两层测试来确保功能的正确性：

1.  **解析器合规性测试 (单元测试)**
    *   **文件**: `tests/test_host_collector_windows_parser_compliance.py`
    *   **用途**: 验证解析逻辑是否符合 Schema 定义，不依赖外部环境。
    *   **运行**: `python tests/test_host_collector_windows_parser_compliance.py`

2.  **Elasticsearch 集成测试**
    *   **文件**: `tests/test_host_collector_windows_es_integration.py`
    *   **用途**: 验证解析后的数据能否成功写入 Elasticsearch。
    *   **运行**: `python tests/test_host_collector_windows_es_integration.py`
    *   **注意**: 需要本地运行 ES (可使用 `docker-compose up -d` 启动)。

