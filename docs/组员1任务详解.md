# 组员1任务详解：主机日志采集

## 任务概览
负责 Linux 主机日志的采集、解析与标准化，重点关注系统审计日志（Auditd）和系统日志（Syslog）。

## 1. Filebeat 安装与配置

主机日志采集依赖 Filebeat 将本地日志（Auditd, Syslog）发送到 Elasticsearch。

### 1. Filebeat 安装与配置

### 1.1 安装方式：Docker 容器

为了保证环境的一致性，我们采用 Docker 容器方式运行 Filebeat，而非直接安装在宿主机。

### 1.2 配置步骤

更新 `docker-compose.yml`，Filebeat 会作为其中的一个服务自动启动。

**您只需要执行以下操作：**

1.  **确保 Docker 环境正常**
    ```bash
    # 在项目根目录下
    docker-compose up -d
    ```
    这会自动拉取并启动 `filebeat-collector` 容器。

2.  **配置原理 (已自动处理)**
    *   **配置文件**: 我们创建了 `collector/host_collector/filebeat.yml`，它会被挂载到容器内。
    *   **日志挂载**: 宿主机的 `/var/log` 目录被挂载到容器的 `/hostfs/var/log`，以便 Filebeat 读取宿主机的 auditd 日志。

3.  **验证状态**
    ```bash
    docker ps | grep filebeat
    # 应该能看到 filebeat-collector 正在运行
    ```

### 1.3 验证采集

运行测试脚本，它会模拟主机操作并检查数据是否入库：

```bash
# 备份原配置
sudo mv /etc/filebeat/filebeat.yml /etc/filebeat/filebeat.yml.bak

# 写入新配置 (请确保缩进正确)
sudo tee /etc/filebeat/filebeat.yml > /dev/null <<EOF
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /var/log/audit/audit.log
  pipeline: "auditd-pipeline"

- type: log
  enabled: true
  paths:
    - /var/log/syslog
    - /var/log/messages
  pipeline: "syslog-pipeline"

setup.ilm.enabled: false
setup.template.name: "unified-logs"
setup.template.pattern: "unified-logs-*"
setup.template.overwrite: true

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "unified-logs-%{+yyyy.MM.dd}"
EOF
```

### 1.3 启动服务

```bash
sudo systemctl daemon-reload
sudo systemctl enable filebeat
sudo systemctl start filebeat

# 检查状态
sudo systemctl status filebeat
```

## 2. Windows 事件日志解析支持 (新增)

为了支持跨平台日志采集，我们在 `collector/host_collector/log_parser.py` 中扩展了对 Windows Event Log (JSON 格式) 的解析支持。

### 2.1 支持的事件类型

目前支持以下三种核心安全事件的解析与映射：

| EventID | 事件类型 | 映射到 UnifiedEvent (Schema) | 关键字段提取 |
| :--- | :--- | :--- | :--- |
| **4624** | 登录成功 | `event.category="authentication"`<br>`event.action="login"` | `user.name` (TargetUserName)<br>`source.ip` (IpAddress) |
| **4688** | 进程创建 | `event.category="process"`<br>`event.action="process_created"` | `process.executable` (NewProcessName)<br>`process.command_line` (CommandLine)<br>`process.pid` (ProcessId) |
| **4663** | 对象访问 | `event.category="file"`<br>`event.action="access"` | `file.path` (ObjectName)<br>`file.name` (文件名) |

### 2.2 代码实现逻辑

*   **统一入口**: `HostLogParser.parse(raw_data, log_type="windows")`
*   **兼容性**: 保留了原有的 `auditd` 解析逻辑，通过 `log_type` 参数区分。
*   **数据清洗**: 自动处理 Windows XML/JSON 中的 `0x` 十六进制 PID 和嵌套的时间戳字段。

### 2.3 测试与演示 (Testing & Demo)

我们提供了单元测试以确保代码质量，以及集成脚本用于演示真实的数据流。

#### 2.3.1 单元测试 (Unit Test)

*   **测试目标**: 验证 Windows 日志解析逻辑是否符合 Schema 定义。
*   **文件路径**: `tests/test_host_collector_windows_parser_compliance.py`
*   **运行方式**: 
    ```bash
    python tests/test_host_collector_windows_parser_compliance.py
    ```

#### 2.3.2 集成演示 (Integration Demo)

本环节将模拟从日志生成到 Kibana 可视化的全过程。

**步骤 1: 数据注入 (Data Injection)**

运行集成测试脚本，它会模拟 Windows 安全日志并实时写入 Elasticsearch：

*   **脚本路径**: `tests/test_host_collector_windows_es_integration.py`
*   **执行命令**:
    ```bash
    python3 tests/test_host_collector_windows_es_integration.py
    ```
*   **预期结果**: 控制台输出 `Ingested Event... Result: created`，表明数据已成功解析并持久化写入到 Elasticsearch。

**步骤 2: Kibana 配置 (Kibana Configuration)**

在 Kibana 中创建一个 Data View 以查看日志数据。

*   **路径**: Stack Management -> Data Views -> Create data view
*   **名称/匹配模式 (Name/Pattern)**: `unified-logs-*`
    *   说明: 这匹配了我们组的统一命名规范 `unified-logs-{YYYY.MM.DD}`。
*   **时间戳字段 (Timestamp field)**: 选择 `@timestamp`

**步骤 3: 场景验证 (Search Scenarios)**

使用以下 Lucene 查询语句来验证我们注入的 3 个模拟安全事件：

**1. Administrator Login (管理员登录)**
*   **Query**: `user.name:"Umut_Admin"`
*   **Description**: 验证 Event ID 4624 (登录成功) 的解析正确性。

**2. Suspicious PowerShell Execution (可疑命令执行)**
*   **Query**: `process.name:"powershell.exe"`
*   **Description**: 验证 Event ID 4688 (进程创建) 及其命令行参数的捕获。

**3. Sensitive File Access (敏感文件访问)**
*   **Query**: `file.name:"project.docx"`
*   **Description**: 验证 Event ID 4663 (文件访问) 的监控能力。

